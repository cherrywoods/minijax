{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adbf4cd-063a-4de4-a7b0-5ec2c09c325d",
   "metadata": {},
   "source": [
    "# MNIST using minijax\n",
    "Train a multi-layered perceptron (MLP) for MNIST handwritten digit recognition. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "213bce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from minijax.compute_graph import make_graph\n",
    "from minijax.core import add, div, mul, sqrt, square, sub\n",
    "from minijax.eval import Array, ones, zeros\n",
    "from minijax.grad import value_and_grad\n",
    "from minijax.jit import jit\n",
    "from minijax.nested_containers import map_structure\n",
    "from minijax.nn import cross_entropy, init_mlp, mlp\n",
    "from minijax.vmap import vmap\n",
    "from mnist_dataset import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b94d5e-dfca-4841-8ce5-7f440a142583",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a62af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 784  # flat 28x28 images\n",
    "layers = [128, 10]  # 10 classes\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "params = init_mlp(in_size, layers, rng_key=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a404131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([ 1.9688427   0.02259841 -0.05577275  0.72828809  0.62225548  2.10965871\n",
      "       -1.57625537 -1.56898686  0.32959784  1.34568429])\n"
     ]
    }
   ],
   "source": [
    "x = ones((28, 28))\n",
    "y = mlp(x, params)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fa03c-e4ba-42fb-b8b4-4de2691e9ed7",
   "metadata": {},
   "source": [
    "## vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082b828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([[ 1.9688427   0.02259841 -0.05577275  0.72828809  0.62225548  2.10965871\n",
      "        -1.57625537 -1.56898686  0.32959784  1.34568429]\n",
      "       [ 0.          0.          0.          0.          0.          0.\n",
      "         0.          0.          0.          0.        ]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = vmap(mlp, (0, None))\n",
    "x = Array([[1.0] * in_size, [0.0] * in_size])\n",
    "y = model(x, params)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31701a3-df5d-4838-a956-51b58337794d",
   "metadata": {},
   "source": [
    "## make_graph\n",
    "Let's inspect the compute graph of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed340b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: a[2, 784] b[784, 128] c[128] d[128, 10] e[10]\n",
      "  f[2, 784] = reshape[new_shape: (2, -1)] a[2, 784]\n",
      "  g[2, 128] = dot f[2, 784] b[784, 128]\n",
      "  h[2, 128] = add g[2, 128] c[128]\n",
      "  i[2, 128] = relu h[2, 128]\n",
      "  j[2, 10] = dot i[2, 128] d[128, 10]\n",
      "  k[2, 10] = add j[2, 10] e[10]\n",
      "output: k[2, 10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cg = make_graph(model)(x, params)\n",
    "print(cg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63282cf9-4729-4d35-bf0c-38affad3c3a1",
   "metadata": {},
   "source": [
    "## Loss & grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "499d0c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Array(1.8139983104301463)\n",
      "Gradients: (Array([[ 0.01794723 -0.02101453 -0.02960505 ...  0.0151266  -0.00292274\n",
      "         0.00611423]\n",
      "       [ 0.          0.          0.         ...  0.          0.\n",
      "         0.        ]]), Array([[0.66270576 1.63582791 1.67501349 1.28298307 1.33599937 0.59229776\n",
      "        2.4352548  2.43162054 1.4823282  0.97428497]\n",
      "       [1.15129255 1.15129255 1.15129255 1.15129255 1.15129255 1.15129255\n",
      "        1.15129255 1.15129255 1.15129255 1.15129255]]), [{'weight': Array([[ 0.         -0.09364378  0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       [ 0.         -0.09364378  0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       [ 0.         -0.09364378  0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       ...\n",
      "       [ 0.         -0.09364378  0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       [ 0.         -0.09364378  0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       [ 0.         -0.09364378  0.         ...  0.          0.\n",
      "         0.        ]]), 'bias': Array([ 0.         -0.09364378  0.          0.         -0.03407749  0.\n",
      "        0.          0.          0.04929716 -0.09273203 -0.00849418  0.05227119\n",
      "       -0.05129586  0.10121295  0.          0.          0.          0.06734696\n",
      "        0.          0.         -0.01216722  0.          0.          0.\n",
      "        0.          0.0096808   0.          0.          0.          0.\n",
      "        0.01456762  0.         -0.03432057  0.          0.          0.\n",
      "        0.0731849   0.04739604 -0.0101605   0.         -0.04103837  0.07846982\n",
      "        0.          0.0270404  -0.06625133  0.02417849  0.          0.\n",
      "        0.08759302  0.         -0.10108908  0.05031686 -0.00241091  0.\n",
      "        0.          0.01800156  0.06394534  0.05031716  0.07702364  0.03886016\n",
      "        0.         -0.02416503  0.          0.          0.          0.03814683\n",
      "       -0.0288311   0.          0.01625935 -0.05570408  0.          0.\n",
      "        0.         -0.08681608  0.          0.01679271  0.         -0.06897467\n",
      "        0.         -0.00294062  0.          0.07019844  0.02313627  0.\n",
      "        0.          0.07375716  0.         -0.02529603  0.          0.\n",
      "        0.         -0.0071939   0.03777067 -0.07200466  0.042474   -0.03882685\n",
      "        0.04224186 -0.04628896 -0.04796121  0.          0.          0.06110766\n",
      "        0.00629956 -0.04374477  0.068508   -0.02297599  0.06589291 -0.00503771\n",
      "        0.         -0.06836399  0.          0.          0.          0.01864308\n",
      "        0.          0.          0.          0.         -0.05089966 -0.0647397\n",
      "        0.          0.          0.035703    0.          0.          0.\n",
      "        0.          0.        ])}, {'weight': Array([[ 0.          0.          0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       [-0.73182534  0.03781534  0.03496487 ...  0.0076993   0.05140392\n",
      "         0.14199628]\n",
      "       [ 0.          0.          0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       ...\n",
      "       [ 0.          0.          0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       [ 0.          0.          0.         ...  0.          0.\n",
      "         0.        ]\n",
      "       [ 0.          0.          0.         ...  0.          0.\n",
      "         0.        ]]), 'bias': Array([-0.3171532  -0.43102823  0.0675417   0.08842245  0.08455697  0.20293494\n",
      "        0.05383473  0.0538627   0.07578909  0.12123884])}])\n"
     ]
    }
   ],
   "source": [
    "def loss(x, y_true, params):\n",
    "    y_pred = model(x, params)\n",
    "    return cross_entropy(y_pred, y_true)\n",
    "\n",
    "\n",
    "# One-hot encoded class labels\n",
    "# => vectors are 1.0 at the correct class\n",
    "y_true = Array(\n",
    "    [\n",
    "        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    ]\n",
    ")\n",
    "loss_val, grads = value_and_grad(loss)(x, y_true, params)\n",
    "\n",
    "print(\"Loss:\", loss_val)\n",
    "print(\"Gradients:\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d43446",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MNIST dataset...\")\n",
    "train_images, train_labels, test_images, test_labels = load_mnist()\n",
    "print(f\"Training set: {train_images.shape} images, {train_labels.shape} labels\")\n",
    "print(f\"Test set: {test_images.shape} images, {test_labels.shape} labels\")\n",
    "\n",
    "train_images[0], train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fc555-b04e-430b-9e14-828ddaf3de55",
   "metadata": {},
   "source": [
    "## Training with jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y_true, params):  # Calculate accuracy in numpy. It's only for logging.\n",
    "    y_pred = model(x, params)\n",
    "    y_true, y_pred = y_true.array, y_pred.array\n",
    "    return np.mean(np.argmax(y_pred, axis=-1) == np.argmax(y_true, axis=-1))\n",
    "\n",
    "\n",
    "def adam(params, grads, opt_state, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    def m_update(g, m_prev):\n",
    "        return add(mul(Array(beta1), m_prev), mul(Array(1 - beta1), g))\n",
    "\n",
    "    def v_update(g, v_prev):\n",
    "        return add(mul(Array(beta2), v_prev), mul(Array(1 - beta2), square(g)))\n",
    "\n",
    "    m_prevs, v_prevs, beta1powtm1, beta2powtm2 = opt_state\n",
    "    # map_structure applies a function to each array in a nested container, e.g. the grads list\n",
    "    m_new = map_structure(m_update, grads, m_prevs)\n",
    "    v_new = map_structure(v_update, grads, v_prevs)\n",
    "    beta1powt = mul(beta1powtm1, Array(beta1))\n",
    "    beta2powt = mul(beta2powtm2, Array(beta2))\n",
    "\n",
    "    def param_update(p, m, v):\n",
    "        m_hat = div(m, sub(Array(1), beta1powt))\n",
    "        v_hat = div(v, sub(Array(1), beta2powt))\n",
    "        return sub(p, mul(Array(lr), div(m_hat, add(sqrt(v_hat), Array(eps)))))\n",
    "\n",
    "    new_params = map_structure(param_update, params, m_new, v_new)\n",
    "    return new_params, (m_new, v_new, beta1powt, beta2powt)\n",
    "\n",
    "\n",
    "def init_adam_state(params):\n",
    "    m = map_structure(lambda p: zeros(p.shape), params)\n",
    "    v = map_structure(lambda p: zeros(p.shape), params)\n",
    "    beta1powt = Array(1)\n",
    "    beta2powt = Array(1)\n",
    "    return m, v, beta1powt, beta2powt\n",
    "\n",
    "\n",
    "@jit  # this will make things only a little faster\n",
    "def train_step(x, y_true, params, opt_state):\n",
    "    loss_val, (_, _, param_grads) = value_and_grad(loss)(x, y_true, params)\n",
    "    new_params, new_opt_state = adam(params, param_grads, opt_state, lr=learning_rate)\n",
    "    return new_params, new_opt_state, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b076966",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = init_adam_state(params)\n",
    "\n",
    "epoch_len = train_images.shape[0] // batch_size\n",
    "np_rng = np.random.default_rng(1)\n",
    "for t in range(num_epochs):\n",
    "    rand_perm = np_rng.permutation(train_images.shape[0])\n",
    "    loss_vals = []\n",
    "    for i, batch_idx in enumerate(it.batched(rand_perm, batch_size)):\n",
    "        x, y = Array(train_images[batch_idx, :]), Array(train_labels[batch_idx, :])\n",
    "        params, opt_state, loss_val = train_step(x, y, params, opt_state)\n",
    "\n",
    "        loss_vals.append(loss_val.array.item())\n",
    "        if i % 100 == 99:\n",
    "            avg_loss = sum(loss_vals) / len(loss_vals)\n",
    "            loss_vals = []\n",
    "            print(f\"[Epoch {t + 1}, {(i + 1) / epoch_len:3.0%}]: loss = {avg_loss:.4f}\")\n",
    "\n",
    "    test_loss = loss(Array(test_images), Array(test_labels), params).array.item()\n",
    "    test_accuracy = accuracy(Array(test_images), Array(test_labels), params).item()\n",
    "    print(f\"Epoch {t + 1}: test loss: {test_loss:.4f}, test accuracy: {test_accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
